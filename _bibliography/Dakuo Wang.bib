@inproceedings{adninExaminingStudentTeacher2025,
  title = {Examining {{Student}} and {{Teacher Perspectives}} on {{Undisclosed Use}} of {{Generative AI}} in {{Academic Work}}},
  booktitle = {Proceedings of the 2025 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Adnin, Rudaiba and Pandkar, Atharva and Yao, Bingsheng and Wang, Dakuo and Das, Maitraye},
  year = 2025,
  month = apr,
  pages = {1--17},
  publisher = {ACM},
  address = {Yokohama Japan},
  doi = {10.1145/3706598.3713393},
  urldate = {2025-11-04},
  isbn = {979-8-4007-1394-1},
  langid = {english}
}

@article{caiPaniniqaEnhancingPatient2023,
  title = {Paniniqa: {{Enhancing}} Patient Education through Interactive Question Answering},
  shorttitle = {Paniniqa},
  author = {Cai, Pengshan and Yao, Zonghai and Liu, Fei and Wang, Dakuo and Reilly, Meghan and Zhou, Huixue and Li, Lingxi and Cao, Yi and Kapoor, Alok and Bajracharya, Adarsha},
  year = 2023,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {11},
  pages = {1518--1536},
  publisher = {MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA \dots},
  url = {https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00616/118717},
  urldate = {2025-11-04}
}

@inproceedings{cardenasDontGetToo2023,
  title = {`{{Don}}'t {{Get Too Technical}} with {{Me}}': {{A Discourse Structure-Based Framework}} for {{Automatic Science Journalism}}},
  shorttitle = {`{{Don}}'t {{Get Too Technical}} with {{Me}}'},
  booktitle = {Proceedings of the 2023 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Cardenas, Ronald and Yao, Bingsheng and Wang, Dakuo and Hou, Yufang},
  year = 2023,
  pages = {1186--1202},
  url = {https://aclanthology.org/2023.emnlp-main.76/},
  urldate = {2025-11-04}
}

@misc{chanceWillPrinceGet2025,
  title = {Will the {{Prince Get True Love}}'s {{Kiss}}? {{On}} the {{Model Sensitivity}} to {{Gender Perturbation}} over {{Fairytale Texts}}},
  shorttitle = {Will the {{Prince Get True Love}}'s {{Kiss}}?},
  author = {Chance, Christina and Yin, Da and Wang, Dakuo and Chang, Kai-Wei},
  year = 2025,
  month = apr,
  number = {arXiv:2310.10865},
  eprint = {2310.10865},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.10865},
  urldate = {2025-11-04},
  abstract = {In this paper, we study whether language models are affected by learned gender stereotypes during the comprehension of stories. Specifically, we investigate how models respond to gender stereotype perturbations through counterfactual data augmentation. Focusing on Question Answering (QA) tasks in fairytales, we modify the FairytaleQA dataset by swapping gendered character information and introducing counterfactual gender stereotypes during training. This allows us to assess model robustness and examine whether learned biases influence story comprehension. Our results show that models exhibit slight performance drops when faced with gender perturbations in the test set, indicating sensitivity to learned stereotypes. However, when fine-tuned on counterfactual training data, models become more robust to anti-stereotypical narratives. Additionally, we conduct a case study demonstrating how incorporating counterfactual anti-stereotype examples can improve inclusivity in downstream applications.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{chanHumanLLMBasedVoice2024,
  title = {Human and {{LLM-Based Voice Assistant Interaction}}: {{An Analytical Framework}} for {{User Verbal}} and {{Nonverbal Behaviors}}},
  shorttitle = {Human and {{LLM-Based Voice Assistant Interaction}}},
  author = {Chan, Szeyi and Fu, Shihan and Li, Jiachen and Yao, Bingsheng and Desai, Smit and Prpa, Mirjana and Wang, Dakuo},
  year = 2024,
  month = sep,
  number = {arXiv:2408.16465},
  eprint = {2408.16465},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.16465},
  urldate = {2025-11-04},
  abstract = {Recent progress in large language model (LLM) technology has significantly enhanced the interaction experience between humans and voice assistants (VAs). This project aims to explore a user's continuous interaction with LLM-based VA (LLM-VA) during a complex task. We recruited 12 participants to interact with an LLM-VA during a cooking task, selected for its complexity and the requirement for continuous interaction. We observed that users show both verbal and nonverbal behaviors, though they know that the LLM-VA can not capture those nonverbal signals. Despite the prevalence of nonverbal behavior in human-human communication, there is no established analytical methodology or framework for exploring it in human-VA interactions. After analyzing 3 hours and 39 minutes of video recordings, we developed an analytical framework with three dimensions: 1) behavior characteristics, including both verbal and nonverbal behaviors, 2) interaction stages--exploration, conflict, and integration--that illustrate the progression of user interactions, and 3) stage transition throughout the task. This analytical framework identifies key verbal and nonverbal behaviors that provide a foundation for future research and practical applications in optimizing human and LLM-VA interactions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Human-Computer Interaction}
}

@article{chanMangoMangoHow2025,
  title = {"{{Mango Mango}}, {{How}} to {{Let The Lettuce Dry Without A Spinner}}?": {{Exploring User Perceptions}} of {{Using An LLM-Based Conversational Assistant Toward Cooking Partner}}},
  shorttitle = {"{{Mango Mango}}, {{How}} to {{Let The Lettuce Dry Without A Spinner}}?},
  author = {Chan, Szeyi and Li, Jiachen and Yao, Bingsheng and Mahmood, Amama and Huang, Chien-Ming and Jimison, Holly and Mynatt, Elizabeth D. and Wang, Dakuo},
  year = 2025,
  month = oct,
  journal = {Proceedings of the ACM on Human-Computer Interaction},
  volume = {9},
  number = {7},
  pages = {1--35},
  issn = {2573-0142},
  doi = {10.1145/3757442},
  urldate = {2025-11-04},
  abstract = {The rapid advancement of Large Language Models (LLMs) has created numerous potentials for integration with conversational assistants (CAs) assisting people in their daily tasks, particularly due to their extensive flexibility. However, users' real-world experiences interacting with these assistants remain unexplored. In this research, we chose cooking, a complex daily task, as a scenario to explore people's successful and unsatisfactory experiences while receiving assistance from an LLM-based CA,               Mango Mango               . We discovered that participants value the system's ability to offer customized instructions based on context, provide extensive information beyond the recipe, and assist them in dynamic task planning. However, users expect the system to be more adaptive to oral conversation and provide more suggestive responses to keep them actively involved. Recognizing that users began treating our LLM-CA as a personal assistant or even a partner rather than just a recipe-reading tool, we propose five design considerations for future development.},
  langid = {english}
}

@inproceedings{chenEvaluatingLLMAgents2024,
  title = {Evaluating the {{LLM}} Agents for Simulating Humanoid Behavior},
  booktitle = {{{CHI}} Conference {{proceedingsCHI Conference}}},
  author = {Chen, Chaoran and Yao, Bingsheng and Ye, Yanfang and Wang, Dakuo and Li, Toby Jia-Jun},
  year = 2024,
  publisher = {The ACM Conference on Human Factors in Computing Systems-HEAL Workshop (HEAL \dots},
  url = {https://par.nsf.gov/servlets/purl/10581238},
  urldate = {2025-11-04}
}

@misc{chenStorySparkQAExpertAnnotatedQA2024,
  title = {{{StorySparkQA}}: {{Expert-Annotated QA Pairs}} with {{Real-World Knowledge}} for {{Children}}'s {{Story-Based Learning}}},
  shorttitle = {{{StorySparkQA}}},
  author = {Chen, Jiaju and Lu, Yuxuan and Zhang, Shao and Yao, Bingsheng and Dong, Yuanzhe and Xu, Ying and Li, Yunyao and Wang, Qianwen and Wang, Dakuo and Sun, Yuling},
  year = 2024,
  month = oct,
  number = {arXiv:2311.09756},
  eprint = {2311.09756},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.09756},
  urldate = {2025-11-04},
  abstract = {Interactive story reading is a common parent-child activity, where parents expect to teach both language skills and real-world knowledge beyond the story. While increasing storytelling and reading systems have been developed for this activity, they often fail to infuse real-world knowledge into the conversation. This limitation can be attributed to the existing question-answering (QA) datasets used for children's education, upon which the systems are built, failing to capture the nuances of how education experts think when conducting interactive story reading activities. To bridge this gap, we design an annotation framework, empowered by existing knowledge graph to capture experts' annotations and thinking process, and leverage this framework to construct StorySparkQA dataset, which comprises 5,868 expert-annotated QA pairs with real-world knowledge. We conduct automated and human expert evaluations across various QA pair generation settings to demonstrate that our StorySparkQA can effectively support models in generating QA pairs that target real-world knowledge beyond story content. StorySparkQA is available at https://huggingface.co/datasets/NEU-HAI/StorySparkQA.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{desaiMetaphorFluidConversationDesign2025,
  title = {Toward {{Metaphor-Fluid Conversation Design}} for {{Voice User Interfaces}}},
  author = {Desai, Smit and Chin, Jessie and Wang, Dakuo and Cowan, Benjamin and Twidale, Michael},
  year = 2025,
  month = oct,
  number = {arXiv:2502.11554},
  eprint = {2502.11554},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.11554},
  urldate = {2025-11-04},
  abstract = {Metaphors play a critical role in shaping user experiences with Voice User Interfaces (VUIs), yet existing designs often rely on static, human-centric metaphors that fail to adapt to diverse contexts and user needs. This paper introduces Metaphor-Fluid Design, a novel approach that dynamically adjusts metaphorical representations based on conversational use-contexts. We compare this approach to a Default VUI, which characterizes the present implementation of commercial VUIs commonly designed around the persona of an assistant, offering a uniform interaction style across contexts. In Study 1 (N=130), metaphors were mapped to four key use-contexts-commands, information seeking, sociality, and error recovery-along the dimensions of formality and hierarchy, revealing distinct preferences for task-specific metaphorical designs. Study 2 (N=91) evaluates a Metaphor-Fluid VUI against a Default VUI, showing that the Metaphor-Fluid VUI enhances perceived intention to adopt, enjoyment, and likability by aligning better with user expectations for different contexts. However, individual differences in metaphor preferences highlight the need for personalization. These findings challenge the one-size-fits-all paradigm of VUI design and demonstrate the potential of Metaphor-Fluid Design to create more adaptive and engaging human-AI interactions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Emerging Technologies,Computer Science - Human-Computer Interaction}
}

@misc{fuItFeltWas2025,
  title = {"{{It Felt Like I Was Left}} in the {{Dark}}": {{Exploring Information Needs}} and {{Design Opportunities}} for {{Family Caregivers}} of {{Older Adult Patients}} in {{Critical Care Settings}}},
  shorttitle = {"{{It Felt Like I Was Left}} in the {{Dark}}"},
  author = {Fu, Shihan and Yao, Bingsheng and Desai, Smit and Hu, Yuqi and Sun, Yuling and Stonbraker, Samantha and Gao, Yanjun and Goldberg, Elizabeth M. and Wang, Dakuo},
  year = 2025,
  month = sep,
  number = {arXiv:2502.05115},
  eprint = {2502.05115},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.05115},
  urldate = {2025-11-04},
  abstract = {Older adult patients constitute a rapidly growing subgroup of Intensive Care Unit (ICU) patients. In these situations, their family caregivers are expected to represent the unconscious patients to access and interpret patients' medical information. However, caregivers currently have to rely on overloaded clinicians for information updates and typically lack the health literacy to understand complex medical information. Our project aims to explore the information needs of caregivers of ICU older adult patients, from which we can propose design opportunities to guide future AI systems. The project begins with formative interviews with 11 caregivers to identify their challenges in accessing and interpreting medical information; From these findings, we then synthesize design requirements and propose an AI system prototype to cope with caregivers' challenges. The system prototype has two key features: a timeline visualization to show the AI extracted and summarized older adult patients' key medical events; and an LLM-based chatbot to provide context-aware informational support. We conclude our paper by reporting on the follow-up user evaluation of the system and discussing future AI-based systems for ICU caregivers of older adults.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction}
}

@inproceedings{goelPromotingProsocialityMicroacts2025,
  title = {Promoting {{Prosociality}} via {{Micro-acts}} of {{Joy}}: {{A Large-Scale Well-Being Intervention Study}}},
  shorttitle = {Promoting {{Prosociality}} via {{Micro-acts}} of {{Joy}}},
  booktitle = {Proceedings of the 2025 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Goel, Hitesh and Park, Yoobin and Liou, Jin and Guevarra, Darwin A and Callahan, Peggy and Smith, Jolene and Yao, Bingsheng and Wang, Dakuo and Liu, Xin and McDuff, Daniel and Elhadad, Noemie and {Simon-Thomas}, Emiliana and Epel, Elissa and Xu, Xuhai},
  year = 2025,
  month = apr,
  pages = {1--28},
  publisher = {ACM},
  address = {Yokohama Japan},
  doi = {10.1145/3706598.3713947},
  urldate = {2025-11-04},
  isbn = {979-8-4007-1394-1},
  langid = {english}
}

@inproceedings{goyalSHAI2023Workshop2023,
  title = {{{SHAI}} 2023: {{Workshop}} on {{Designing}} for {{Safety}} in {{Human-AI Interactions}}},
  shorttitle = {{{SHAI}} 2023},
  booktitle = {28th {{International Conference}} on {{Intelligent User Interfaces}}},
  author = {Goyal, Nitesh and Hong, Sungsoo Ray and Mandryk, Regan L and Li, Toby Jia-Jun and Luther, Kurt and Wang, Dakuo},
  year = 2023,
  month = mar,
  pages = {199--201},
  publisher = {ACM},
  address = {Sydney NSW Australia},
  doi = {10.1145/3581754.3584169},
  urldate = {2025-11-04},
  isbn = {979-8-4007-0107-8},
  langid = {english}
}

@misc{isazaAreFairyTales2023,
  title = {Are {{Fairy Tales Fair}}? {{Analyzing Gender Bias}} in {{Temporal Narrative Event Chains}} of {{Children}}'s {{Fairy Tales}}},
  shorttitle = {Are {{Fairy Tales Fair}}?},
  author = {Isaza, Paulina Toro and Xu, Guangxuan and Oloko, Akintoye and Hou, Yufang and Peng, Nanyun and Wang, Dakuo},
  year = 2023,
  month = may,
  number = {arXiv:2305.16641},
  eprint = {2305.16641},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.16641},
  urldate = {2025-11-04},
  abstract = {Social biases and stereotypes are embedded in our culture in part through their presence in our stories, as evidenced by the rich history of humanities and social science literature analyzing such biases in children stories. Because these analyses are often conducted manually and at a small scale, such investigations can benefit from the use of more recent natural language processing methods that examine social bias in models and data corpora. Our work joins this interdisciplinary effort and makes a unique contribution by taking into account the event narrative structures when analyzing the social bias of stories. We propose a computational pipeline that automatically extracts a story's temporal narrative verb-based event chain for each of its characters as well as character attributes such as gender. We also present a verb-based event annotation scheme that can facilitate bias analysis by including categories such as those that align with traditional stereotypes. Through a case study analyzing gender bias in fairy tales, we demonstrate that our framework can reveal bias in not only the unigram verb-based events in which female and male characters participate but also in the temporal narrative order of such event participation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@inproceedings{jahanbakhshExploringUsePersonalized2023,
  title = {Exploring the {{Use}} of {{Personalized AI}} for {{Identifying Misinformation}} on {{Social Media}}},
  booktitle = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Jahanbakhsh, Farnaz and Katsis, Yannis and Wang, Dakuo and Popa, Lucian and Muller, Michael},
  year = 2023,
  month = apr,
  pages = {1--27},
  publisher = {ACM},
  address = {Hamburg Germany},
  doi = {10.1145/3544548.3581219},
  urldate = {2025-11-04},
  isbn = {978-1-4503-9421-5},
  langid = {english}
}

@article{jingDeepLearningAssisted2023,
  title = {Deep {{Learning}}--{{Assisted Gait Parameter Assessment}} for {{Neurodegenerative Diseases}}: {{Model Development}} and {{Validation}}},
  shorttitle = {Deep {{Learning}}--{{Assisted Gait Parameter Assessment}} for {{Neurodegenerative Diseases}}},
  author = {Jing, Yu and Qin, Peinuan and Fan, Xiangmin and Qiang, Wei and Wencheng, Zhu and Sun, Wei and Tian, Feng and Wang, Dakuo},
  year = 2023,
  journal = {Journal of medical Internet research},
  volume = {25},
  pages = {e46427},
  publisher = {JMIR Publications Toronto, Canada},
  url = {https://www.jmir.org/2023/1/e46427/},
  urldate = {2025-11-04}
}

@article{kruseZeroshotLargeLanguage2025,
  title = {Zero-Shot Large Language Models for Long Clinical Text Summarization with Temporal Reasoning},
  author = {Kruse, Maya and Hu, Shiyue and Derby, Nicholas and Wu, Yifu and Stonbraker, Samantha and Yao, Bingsheng and Wang, Dakuo and Goldberg, Elizabeth and Gao, Yanjun},
  year = 2025,
  journal = {medRxiv},
  pages = {2025--07},
  publisher = {Cold Spring Harbor Laboratory Press},
  url = {https://www.medrxiv.org/content/10.1101/2025.07.21.25331947.abstract},
  urldate = {2025-11-04}
}

@inproceedings{lamModelSketchingCentering2023,
  title = {Model {{Sketching}}: {{Centering Concepts}} in {{Early-Stage Machine Learning Model Design}}},
  shorttitle = {Model {{Sketching}}},
  booktitle = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Lam, Michelle S. and Ma, Zixian and Li, Anne and Freitas, Izequiel and Wang, Dakuo and Landay, James A. and Bernstein, Michael S.},
  year = 2023,
  month = apr,
  pages = {1--24},
  publisher = {ACM},
  address = {Hamburg Germany},
  doi = {10.1145/3544548.3581290},
  urldate = {2025-11-04},
  isbn = {978-1-4503-9421-5},
  langid = {english}
}

@misc{leiWatchGuardianEnablingUserDefined2025,
  title = {{{WatchGuardian}}: {{Enabling User-Defined Personalized Just-in-Time Intervention}} on {{Smartwatch}}},
  shorttitle = {{{WatchGuardian}}},
  author = {Lei, Ying and Cao, Yancheng and Wang, Will and Dong, Yuanzhe and Yin, Changchang and Cao, Weidan and Zhang, Ping and Yang, Jingzhen and Yao, Bingsheng and Peng, Yifan and Weng, Chunhua and Auerbach, Randy and Mamykina, Lena and Wang, Dakuo and Wang, Yuntao and Xu, Xuhai},
  year = 2025,
  month = feb,
  number = {arXiv:2502.05783},
  eprint = {2502.05783},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.05783},
  urldate = {2025-11-04},
  abstract = {While just-in-time interventions (JITIs) have effectively targeted common health behaviors, individuals often have unique needs to intervene in personal undesirable actions that can negatively affect physical, mental, and social well-being. We present WatchGuardian, a smartwatch-based JITI system that empowers users to define custom interventions for these personal actions with a small number of samples. For the model to detect new actions based on limited new data samples, we developed a few-shot learning pipeline that finetuned a pre-trained inertial measurement unit (IMU) model on public hand-gesture datasets. We then designed a data augmentation and synthesis process to train additional classification layers for customization. Our offline evaluation with 26 participants showed that with three, five, and ten examples, our approach achieved an average accuracy of 76.8\%, 84.7\%, and 87.7\%, and an F1 score of 74.8\%, 84.2\%, and 87.2\% We then conducted a four-hour intervention study to compare WatchGuardian against a rule-based intervention. Our results demonstrated that our system led to a significant reduction by 64.0 +- 22.6\% in undesirable actions, substantially outperforming the baseline by 29.0\%. Our findings underscore the effectiveness of a customizable, AI-driven JITI system for individuals in need of behavioral intervention in personal undesirable actions. We envision that our work can inspire broader applications of user-defined personalized intervention with advanced AI solutions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning}
}

@inproceedings{liHumanCenteredPrivacyResearch2024a,
  title = {Human-{{Centered Privacy Research}} in the {{Age}} of {{Large Language Models}}},
  booktitle = {Extended {{Abstracts}} of the {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Li, Tianshi and Das, Sauvik and Lee, Hao-Ping (Hank) and Wang, Dakuo and Yao, Bingsheng and Zhang, Zhiping},
  year = 2024,
  month = may,
  pages = {1--4},
  publisher = {ACM},
  address = {Honolulu HI USA},
  doi = {10.1145/3613905.3643983},
  urldate = {2025-11-04},
  isbn = {979-8-4007-0331-7},
  langid = {english}
}

@misc{linSFTDoesntAlways2025,
  title = {{{SFT Doesn}}'t {{Always Hurt General Capabilities}}: {{Revisiting Domain-Specific Fine-Tuning}} in {{LLMs}}},
  shorttitle = {{{SFT Doesn}}'t {{Always Hurt General Capabilities}}},
  author = {Lin, Jiacheng and Wang, Zhongruo and Qian, Kun and Wang, Tian and Srinivasan, Arvind and Zeng, Hansi and Jiao, Ruochen and Zhou, Xie and Gesi, Jiri and Wang, Dakuo and Guo, Yufan and Zhong, Kai and Zhang, Weiqi and Sanghavi, Sujay and Chen, Changyou and Yun, Hyokun and Li, Lihong},
  year = 2025,
  month = sep,
  number = {arXiv:2509.20758},
  eprint = {2509.20758},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2509.20758},
  urldate = {2025-11-04},
  abstract = {Supervised Fine-Tuning (SFT) on domain-specific datasets is a common approach to adapt Large Language Models (LLMs) to specialized tasks but is often believed to degrade their general capabilities. In this work, we revisit this trade-off and present both empirical and theoretical insights. First, we show that SFT does not always hurt: using a smaller learning rate can substantially mitigate general performance degradation while preserving comparable target-domain performance. We then provide a theoretical analysis that explains these phenomena and further motivates a new method, Token-Adaptive Loss Reweighting (TALR). Building on this, and recognizing that smaller learning rates alone do not fully eliminate general-performance degradation in all cases, we evaluate a range of strategies for reducing general capability loss, including L2 regularization, LoRA, model averaging, FLOW, and our proposed TALR. Experimental results demonstrate that while no method completely eliminates the trade-off, TALR consistently outperforms these baselines in balancing domain-specific gains and general capabilities. Finally, we distill our findings into practical guidelines for adapting LLMs to new domains: (i) using a small learning rate to achieve a favorable trade-off, and (ii) when a stronger balance is further desired, adopt TALR as an effective strategy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@inproceedings{liUnderstandingDailyLives2024,
  title = {Understanding the {{Daily Lives}} of {{Older Adults}}: {{Integrating Multi-modal Personal Health Tracking Data}} through {{Visualization}} and {{Large Language Models}}},
  shorttitle = {Understanding the {{Daily Lives}} of {{Older Adults}}},
  booktitle = {Proceedings of the {{AAAI Symposium Series}}},
  author = {Li, Jiachen and Steinberg, Justin and Li, Xiwen and Yao, Bingsheng and Wang, Dakuo and Mynatt, Elizabeth and Mishra, Varun},
  year = 2024,
  volume = {4},
  pages = {173--177},
  url = {https://ojs.aaai.org/index.php/AAAI-SS/article/view/31790},
  urldate = {2025-11-04}
}

@article{liVitalInsightAssisting2024,
  title = {Vital {{Insight}}: {{Assisting Experts}}' {{Sensemaking Process}} of {{Multi-modal Personal Tracking Data Using Visualization}} and {{LLM}}},
  shorttitle = {Vital {{Insight}}},
  author = {Li, Jiachen and Steinberg, Justin and Li, Xiwen and Choube, Akshat and Yao, Bingsheng and Wang, Dakuo and Mynatt, Elizabeth and Mishra, Varun},
  year = 2024,
  journal = {arXiv e-prints},
  pages = {arXiv--2410},
  url = {https://ui.adsabs.harvard.edu/abs/2024arXiv241014879L/abstract},
  urldate = {2025-11-04}
}

@article{luBelievabilityAccurateHuman2025,
  title = {Beyond Believability: {{Accurate}} Human Behavior Simulation with Fine-Tuned Llms},
  shorttitle = {Beyond Believability},
  author = {Lu, Yuxuan and Huang, Jing and Han, Yan and Bei, Bennet and Xie, Yaochen and Wang, Dakuo and Wang, Jessie and He, Qi},
  year = 2025,
  journal = {arXiv preprint arXiv:2503.20749},
  eprint = {2503.20749},
  url = {https://ui.adsabs.harvard.edu/abs/2025arXiv250320749L/abstract},
  urldate = {2025-11-04},
  archiveprefix = {arXiv}
}

@article{luDoesMoreAdvice2024,
  title = {Does {{More Advice Help}}? {{The Effects}} of {{Second Opinions}} in {{AI-Assisted Decision Making}}},
  shorttitle = {Does {{More Advice Help}}?},
  author = {Lu, Zhuoran and Wang, Dakuo and Yin, Ming},
  year = 2024,
  month = apr,
  journal = {Proceedings of the ACM on Human-Computer Interaction},
  volume = {8},
  number = {CSCW1},
  pages = {1--31},
  issn = {2573-0142},
  doi = {10.1145/3653708},
  urldate = {2025-11-04},
  abstract = {AI assistance in decision-making has become popular, yet people's inappropriate reliance on AI often leads to unsatisfactory human-AI collaboration performance. In this paper, through three pre-registered, randomized human subject experiments, we explore whether and how the provision of second opinions may affect decision-makers' behavior and performance in AI-assisted decision-making. We find that if both the AI model's decision recommendation and a second opinion are always presented together, decision-makers reduce their over-reliance on AI while increase their under-reliance on AI, regardless whether the second opinion is generated by a peer or another AI model. However, if decision-makers have the control to decide when to solicit a peer's second opinion, we find that their active solicitations of second opinions have the potential to mitigate over-reliance on AI without inducing increased under-reliance in some cases. We conclude by discussing the implications of our findings for promoting effective human-AI collaborations in decision-making.},
  langid = {english}
}

@inproceedings{luUXAgentLLMAgentBased2025a,
  title = {{{UXAgent}}: {{An LLM Agent-Based Usability Testing Framework}} for {{Web Design}}},
  shorttitle = {{{UXAgent}}},
  booktitle = {Proceedings of the {{Extended Abstracts}} of the {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Lu, Yuxuan and Yao, Bingsheng and Gu, Hansu and Huang, Jing and Wang, Zheshen Jessie and Li, Yang and Gesi, Jiri and He, Qi and Li, Toby Jia-Jun and Wang, Dakuo},
  year = 2025,
  month = apr,
  series = {{{CHI EA}} '25},
  pages = {1--12},
  publisher = {ACM},
  address = {Yokohama Japan},
  doi = {10.1145/3706599.3719729},
  urldate = {2025-11-04},
  abstract = {Usability testing is a fundamental yet challenging research method for user experience (UX) researchers to evaluate a web design. Recent advances in Large Language Model-simulated Agent (LLM Agent) research inspired us to design UXAgent to support UX researchers in evaluating and reiterating their usability testing study design before they conduct the real human-subject study. Our system features an LLM Agent module and a universal browser connector module so that UX researchers can automatically generate thousands of simulated users to test the target website. The system can generate UX study results in qualitative (e.g., interviewing how an agent thinks), quantitative (e.g., \# of actions), and video recording formats for UX researchers to analyze. Through a heuristic user evaluation with five UX researchers, participants praised the innovation of our system but also expressed concerns about the future of UX study with LLM Agents1.},
  isbn = {979-8-4007-1395-8},
  langid = {english}
}

@misc{luWEBSERVBrowserServerEnvironment2025,
  title = {{{WEBSERV}}: {{A Browser-Server Environment}} for {{Efficient Training}} of {{Reinforcement Learning-based Web Agents}} at {{Scale}}},
  shorttitle = {{{WEBSERV}}},
  author = {Lu, Yuxuan and Huang, Jing and Liu, Hui and Gesi, Jiri and Han, Yan and Fu, Shihan and Zheng, Tianqi and Wang, Dakuo},
  year = 2025,
  month = oct,
  number = {arXiv:2510.16252},
  eprint = {2510.16252},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2510.16252},
  urldate = {2025-11-04},
  abstract = {Training and evaluation of Reinforcement Learning (RL) web agents have gained increasing attention, yet a scalable and efficient environment that couples realistic and robust browser-side interaction with controllable server-side state at scale is still missing. Existing environments tend to have one or more of the following issues: they overwhelm policy models with excessive and noisy context; they perform actions non-deterministically without waiting for the UI or network to stabilize; or they cannot scale isolated client-server containers effectively for parallel RL rollouts. We propose WEBSERV, an environment that includes 1) a compact, site-agnostic browser environment that balances context and action complexity, and 2) a scalable RL environment via efficient launching and resetting web-servers to enable scalable RL training and evaluation. We evaluate WEBSERV on the shopping CMS and Gitlab tasks in WebArena, achieving state-of-the-art single-prompt success rates while cutting launch latency by \textasciitilde 5x and storage need by \textasciitilde 240x, with a comparable memory footprint, enabling 200+ concurrent containers on a single host.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@article{mahmoodLlmpoweredConversationalVoice2023,
  title = {Llm-Powered Conversational Voice Assistants: {{Interaction}} Patterns, Opportunities, Challenges, and Design Guidelines},
  shorttitle = {Llm-Powered Conversational Voice Assistants},
  author = {Mahmood, Amama and Wang, Junxiang and Yao, Bingsheng and Wang, Dakuo and Huang, Chien-Ming},
  year = 2023,
  journal = {arXiv preprint arXiv:2309.13879},
  eprint = {2309.13879},
  url = {https://www.sciencedirect.com/science/article/pii/S1071581924001897},
  urldate = {2025-11-04},
  archiveprefix = {arXiv}
}

@misc{pisanoBergeronCombatingAdversarial2024,
  title = {Bergeron: {{Combating Adversarial Attacks}} through a {{Conscience-Based Alignment Framework}}},
  shorttitle = {Bergeron},
  author = {Pisano, Matthew and Ly, Peter and Sanders, Abraham and Yao, Bingsheng and Wang, Dakuo and Strzalkowski, Tomek and Si, Mei},
  year = 2024,
  month = aug,
  number = {arXiv:2312.00029},
  eprint = {2312.00029},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.00029},
  urldate = {2025-11-04},
  abstract = {Research into AI alignment has grown considerably since the recent introduction of increasingly capable Large Language Models (LLMs). Unfortunately, modern methods of alignment still fail to fully prevent harmful responses when models are deliberately attacked. Such vulnerabilities can lead to LLMs being manipulated into generating hazardous content: from instructions for creating dangerous materials to inciting violence or endorsing unethical behaviors. To help mitigate this issue, we introduce Bergeron: a framework designed to improve the robustness of LLMs against attacks without any additional parameter fine-tuning. Bergeron is organized into two tiers; with a secondary LLM acting as a guardian to the primary LLM. This framework better safeguards the primary model against incoming attacks while monitoring its output for any harmful content. Empirical analysis reviews that by using Bergeron to complement models with existing alignment training, we can significantly improve the robustness and safety of multiple, commonly used commercial and open-source LLMs. Specifically, we found that models integrated with Bergeron are, on average, nearly seven times more resistant to attacks compared to models without such support.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Cryptography and Security}
}

@inproceedings{prpaChallengesOpportunitiesLLMBased2024,
  title = {Challenges and {{Opportunities}} of {{LLM-Based Synthetic Personae}} and {{Data}} in {{HCI}}},
  booktitle = {Companion {{Publication}} of the 2024 {{Conference}} on {{Computer-Supported Cooperative Work}} and {{Social Computing}}},
  author = {Prpa, Mirjana and Troiano, Giovanni and Yao, Bingsheng and Li, Toby Jia-Jun and Wang, Dakuo and Gu, Hansu},
  year = 2024,
  month = nov,
  pages = {716--719},
  publisher = {ACM},
  address = {San Jose Costa Rica},
  doi = {10.1145/3678884.3681826},
  urldate = {2025-11-04},
  isbn = {979-8-4007-1114-5},
  langid = {english}
}

@inproceedings{sunLiveStreamingBasedDualTeacherClasses2025,
  title = {Live-{{Streaming-Based Dual-Teacher Classes}} for {{Equitable Education}}: {{Insights}} and {{Challenges From Local Teachers}}' {{Perspective}} in {{Disadvantaged Areas}}},
  shorttitle = {Live-{{Streaming-Based Dual-Teacher Classes}} for {{Equitable Education}}},
  booktitle = {Proceedings of the 2025 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Sun, Yuling and Chen, Jiaju and Zhou, Xiaomu and Ma, Xiaojuan and Yao, Bingsheng and Zhang, Kai and He, Liang and Wang, Dakuo},
  year = 2025,
  month = apr,
  pages = {1--18},
  publisher = {ACM},
  address = {Yokohama Japan},
  doi = {10.1145/3706598.3714232},
  urldate = {2025-11-04},
  isbn = {979-8-4007-1394-1},
  langid = {english}
}

@misc{tangDarkPatternsMeet2025,
  title = {Dark {{Patterns Meet GUI Agents}}: {{LLM Agent Susceptibility}} to {{Manipulative Interfaces}} and the {{Role}} of {{Human Oversight}}},
  shorttitle = {Dark {{Patterns Meet GUI Agents}}},
  author = {Tang, Jingyu and Chen, Chaoran and Li, Jiawen and Zhang, Zhiping and Guo, Bingcan and Khalilov, Ibrahim and Gebreegziabher, Simret Araya and Yao, Bingsheng and Wang, Dakuo and Ye, Yanfang and Li, Tianshi and Xiao, Ziang and Yao, Yaxing and Li, Toby Jia-Jun},
  year = 2025,
  month = sep,
  number = {arXiv:2509.10723},
  eprint = {2509.10723},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2509.10723},
  urldate = {2025-11-04},
  abstract = {The dark patterns, deceptive interface designs manipulating user behaviors, have been extensively studied for their effects on human decision-making and autonomy. Yet, with the rising prominence of LLM-powered GUI agents that automate tasks from high-level intents, understanding how dark patterns affect agents is increasingly important. We present a two-phase empirical study examining how agents, human participants, and human-AI teams respond to 16 types of dark patterns across diverse scenarios. Phase 1 highlights that agents often fail to recognize dark patterns, and even when aware, prioritize task completion over protective action. Phase 2 revealed divergent failure modes: humans succumb due to cognitive shortcuts and habitual compliance, while agents falter from procedural blind spots. Human oversight improved avoidance but introduced costs such as attentional tunneling and cognitive load. Our findings show neither humans nor agents are uniformly resilient, and collaboration introduces new vulnerabilities, suggesting design needs for transparency, adjustable autonomy, and oversight.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction}
}

@article{tanSeatTableEnough2024,
  title = {Is a {{Seat}} at the {{Table Enough}}? {{Engaging Teachers}} and {{Students}} in {{Dataset Specification}} for {{ML}} in {{Education}}},
  shorttitle = {Is a {{Seat}} at the {{Table Enough}}?},
  author = {Tan, Mei and Lee, Hansol and Wang, Dakuo and Subramonyam, Hari},
  year = 2024,
  month = apr,
  journal = {Proceedings of the ACM on Human-Computer Interaction},
  volume = {8},
  number = {CSCW1},
  pages = {1--32},
  issn = {2573-0142},
  doi = {10.1145/3637358},
  urldate = {2025-11-04},
  abstract = {Despite the promises of ML in education, its adoption in the classroom has surfaced numerous issues regarding fairness, accountability, and transparency, as well as concerns about data privacy and student consent. A root cause of these issues is the lack of understanding of the complex dynamics of education, including teacher-student interactions, collaborative learning, and classroom environments. To overcome these challenges and fully utilize the potential of ML in education, software practitioners need to work closely with educators and students to fully understand the context of the data (the backbone of ML applications) and collaboratively define the ML data specifications. To gain a deeper understanding of such a collaborative process, we conduct ten co-design sessions with ML software practitioners, educators, and students. In the sessions, teachers and students work with ML engineers, UX designers, and legal practitioners to define dataset characteristics for a given ML application. We find that stakeholders contextualize data based on their domain and procedural knowledge, proactively design data requirements to mitigate downstream harms and data reliability concerns, and exhibit role-based collaborative strategies and contribution patterns. Further, we find that beyond a seat at the table, meaningful stakeholder participation in ML requires structured supports: defined processes for continuous iteration and co-evaluation, shared contextual data quality standards, and information scaffolds for both technical and non-technical stakeholders to traverse expertise boundaries.},
  langid = {english}
}

@misc{tieLLMsAreImperfect2024,
  title = {{{LLMs}} Are {{Imperfect}}, {{Then What}}? {{An Empirical Study}} on {{LLM Failures}} in {{Software Engineering}}},
  shorttitle = {{{LLMs}} Are {{Imperfect}}, {{Then What}}?},
  author = {Tie, Jiessie and Yao, Bingsheng and Li, Tianshi and Ahmed, Syed Ishtiaque and Wang, Dakuo and Zhou, Shurui},
  year = 2024,
  month = nov,
  number = {arXiv:2411.09916},
  eprint = {2411.09916},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.09916},
  urldate = {2025-11-04},
  abstract = {Software engineers are integrating AI assistants into their workflows to enhance productivity and reduce cognitive strain. However, experiences vary significantly, with some engineers finding large language models (LLMs), like ChatGPT, beneficial, while others consider them counterproductive. Researchers also found that ChatGPT's answers included incorrect information. Given the fact that LLMs are still imperfect, it is important to understand how to best incorporate LLMs into the workflow for software engineering (SE) task completion. Therefore, we conducted an observational study with 22 participants using ChatGPT as a coding assistant in a non-trivial SE task to understand the practices, challenges, and opportunities for using LLMs for SE tasks. We identified the cases where ChatGPT failed, their root causes, and the corresponding mitigation solutions used by users. These findings contribute to the overall understanding and strategies for human-AI interaction on SE tasks. Our study also highlights future research and tooling support directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Software Engineering}
}

@inproceedings{wanBuildingLLMbasedAI2024,
  title = {Building {{LLM-based AI Agents}} in {{Social Virtual Reality}}},
  booktitle = {Extended {{Abstracts}} of the {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Wan, Hongyu and Zhang, Jinda and Suria, Abdulaziz Arif and Yao, Bingsheng and Wang, Dakuo and Coady, Yvonne and Prpa, Mirjana},
  year = 2024,
  month = may,
  pages = {1--7},
  publisher = {ACM},
  address = {Honolulu HI USA},
  doi = {10.1145/3613905.3651026},
  urldate = {2025-11-04},
  isbn = {979-8-4007-0331-7},
  langid = {english}
}

@misc{wangCustomerR1PersonalizedSimulation2025,
  title = {Customer-{{R1}}: {{Personalized Simulation}} of {{Human Behaviors}} via {{RL-based LLM Agent}} in {{Online Shopping}}},
  shorttitle = {Customer-{{R1}}},
  author = {Wang, Ziyi and Lu, Yuxuan and Zhang, Yimeng and Huang, Jing and Wang, Dakuo},
  year = 2025,
  month = oct,
  number = {arXiv:2510.07230},
  eprint = {2510.07230},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2510.07230},
  urldate = {2025-11-04},
  abstract = {Simulating step-wise human behavior with Large Language Models (LLMs) has become an emerging research direction, enabling applications in various practical domains. While prior methods, including prompting, supervised fine-tuning (SFT), and reinforcement learning (RL), have shown promise in modeling step-wise behavior, they primarily learn a population-level policy without conditioning on a user's persona, yielding generic rather than personalized simulations. In this work, we pose a critical question: how can LLM agents better simulate personalized user behavior? We introduce Customer-R1, an RL-based method for personalized, step-wise user behavior simulation in online shopping environments. Our policy is conditioned on an explicit persona, and we optimize next-step rationale and action generation via action correctness reward signals. Experiments on the OPeRA dataset emonstrate that Customer-R1 not only significantly outperforms prompting and SFT-based baselines in next-action prediction tasks, but also better matches users' action distribution, indicating higher fidelity in personalized behavior simulation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@article{wangHumancenteredDesignEvaluation2023,
  title = {Human-Centered Design and Evaluation of {{AI-empowered}} Clinical Decision Support Systems: A Systematic Review},
  shorttitle = {Human-Centered Design and Evaluation of {{AI-empowered}} Clinical Decision Support Systems},
  author = {Wang, Liuping and Zhang, Zhan and Wang, Dakuo and Cao, Weidan and Zhou, Xiaomu and Zhang, Ping and Liu, Jianxing and Fan, Xiangmin and Tian, Feng},
  year = 2023,
  journal = {Frontiers in Computer Science},
  volume = {5},
  pages = {1187299},
  publisher = {Frontiers Media SA},
  url = {https://www.frontiersin.org/journals/computer-science/articles/10.3389/fcomp.2023.1187299/full},
  urldate = {2025-11-04}
}

@article{wangOrganizationalDistanceAlso2022,
  title = {Organizational {{Distance Also Matters}}: {{How Organizational Distance Among Industrial Research Teams Affect Their Research Productivity}}},
  shorttitle = {Organizational {{Distance Also Matters}}},
  author = {Wang, Dakuo and Muller, Michael and Yang, Qian and Wang, Zijun and Tan, Ming and Hobson, Stacy},
  year = 2022,
  month = nov,
  journal = {Proceedings of the ACM on Human-Computer Interaction},
  volume = {6},
  number = {CSCW2},
  pages = {1--18},
  issn = {2573-0142},
  doi = {10.1145/3555554},
  urldate = {2025-11-04},
  abstract = {Geographically distributed teams often face challenges in coordination and collaboration, lowering their productivity. Understanding the relationship between team dispersion and productivity is critical for supporting such teams. Extensive prior research has studied these relations in lab settings or using qualitative measures. This paper extends prior work by contributing an empirical case study in a real-world organization, using quantitative measures. We studied 117 new research project teams from the same discipline within an industrial research lab for 6 months. During this time, all teams shared one goal: submitting research papers to the same target conference. We analyzed these teams' dispersion-related characteristics as well as team productivity. Interestingly, we found little statistical evidence that geographic and time differences relate to team productivity. However, organizational and functional distances are predictive of the productivity of the dispersed teams we studied. We discuss the open research questions these findings revealed and their implications for future research.},
  langid = {english}
}

@inproceedings{wuCardioAIMultimodalAIbased2025,
  title = {{{CardioAI}}: {{A Multimodal AI-based System}} to {{Support Symptom Monitoring}} and {{Risk Prediction}} of {{Cancer Treatment-Induced Cardiotoxicity}}},
  shorttitle = {{{CardioAI}}},
  booktitle = {Proceedings of the 2025 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Wu, Siyi and Cao, Weidan and Fu, Shihan and Yao, Bingsheng and Yang, Ziqi and Yin, Changchang and Mishra, Varun and Addison, Daniel and Zhang, Ping and Wang, Dakuo},
  year = 2025,
  month = apr,
  pages = {1--22},
  publisher = {ACM},
  address = {Yokohama Japan},
  doi = {10.1145/3706598.3714272},
  urldate = {2025-11-04},
  isbn = {979-8-4007-1394-1},
  langid = {english}
}

@misc{wuClinicalChallengesAI2024,
  title = {Clinical {{Challenges}} and {{AI Opportunities}} in {{Decision-Making}} for {{Cancer Treatment-Induced Cardiotoxicity}}},
  author = {Wu, Siyi and Cao, Weidan and Fu, Shihan and Yao, Bingsheng and Yang, Ziqi and Yin, Changchang and Mishra, Varun and Addison, Daniel and Zhang, Ping and Wang, Dakuo},
  year = 2024,
  month = aug,
  number = {arXiv:2408.03586},
  eprint = {2408.03586},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.03586},
  urldate = {2025-11-04},
  abstract = {Cardiotoxicity induced by cancer treatment has become a major clinical concern, affecting the long-term survival and quality of life of cancer patients. Effective clinical decision-making, including the detection of cancer treatment-induced cardiotoxicity and the monitoring of associated symptoms, remains a challenging task for clinicians. This study investigates the current practices and needs of clinicians in the clinical decision making of cancer treatment-induced cardiotoxicity and explores the potential of digital health technologies to support this process. Through semi-structured interviews with seven clinical experts, we identify a three-step decision-making paradigm: 1) symptom identification, 2) diagnostic testing and specialist collaboration, and 3) clinical decision-making and intervention. Our findings highlight the difficulties of diagnosing cardiotoxicity (absence of unified protocols and high variability in symptoms) and monitoring patient symptoms (lacking accurate and timely patient self-reported symptoms). The clinicians also expressed their need for effective early detection tools that can integrate remote patient monitoring capabilities. Based on these insights, we discuss the importance of understanding the dynamic nature of clinical workflows, and the design considerations for future digital tools to support cancer-treatment-induced cardiotoxicity decision-making.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Human-Computer Interaction}
}

@article{wuMaliciousSellingStrategies2023,
  title = {Malicious {{Selling Strategies}} in {{Livestream E-commerce}}: {{A Case Study}} of {{Alibaba}}'s {{Taobao}} and {{ByteDance}}'s {{TikTok}}},
  shorttitle = {Malicious {{Selling Strategies}} in {{Livestream E-commerce}}},
  author = {Wu, Qunfang and Sang, Yisi and Wang, Dakuo and Lu, Zhicong},
  year = 2023,
  month = jun,
  journal = {ACM Transactions on Computer-Human Interaction},
  volume = {30},
  number = {3},
  pages = {1--29},
  issn = {1073-0516, 1557-7325},
  doi = {10.1145/3577199},
  urldate = {2025-11-04},
  abstract = {Due to the limitations imposed by the COVID-19 pandemic, customers have shifted their shopping patterns from offline to online. Livestream shopping has become popular as one of the online shopping media. However, various streamers' malicious selling behaviors have been reported. In this research, we sought to explore streamers' malicious selling strategies and understand how viewers perceive these strategies. First, we recorded 40 livestream shopping sessions from two popular livestream platforms in China---Taobao, and TikTok. We identified 16 malicious selling strategies that were used to deceive, coerce, or manipulate viewers and found that platform designs enhanced nine of the malicious selling strategies. Second, through an interview study with 13 viewers, we report three challenges of overcoming malicious selling in relation to imbalanced power between viewers, streamers, and the platforms. We conclude by discussing the policy and design implications of countering malicious selling.},
  langid = {english}
}

@article{wuSunnieAnthropomorphicLLMbased2024,
  title = {Sunnie: {{An}} Anthropomorphic {{LLM-based}} Conversational Agent for Mental Well-Being Activity Recommendation},
  shorttitle = {Sunnie},
  author = {Wu, Siyi and Han, Feixue and Yao, Bingsheng and Xie, Tianyi and Zhao, Xuan and Wang, Dakuo},
  year = 2024,
  journal = {arXiv e-prints},
  pages = {arXiv--2405},
  url = {https://ui.adsabs.harvard.edu/abs/2024arXiv240513803W/abstract},
  urldate = {2025-11-04}
}

@article{xuMentalLLMLeveragingLarge2023a,
  title = {Mental-{{LLM}}: Leveraging Large Language Models for Mental Health Prediction via Online Text Data. {{arXiv}}},
  shorttitle = {Mental-{{LLM}}},
  author = {Xu, Xuhai and Yao, Bingsheng and Dong, Yuanzhe and Gabriel, Saadia and Yu, Hong and Hendler, James and Ghassemi, Marzyeh and Dey, Anind K. and Wang, Dakuo},
  year = 2023,
  journal = {Preprint posted online on Jul},
  volume = {26},
  url = {https://scholar.google.com/scholar?cluster=4640772408519545607&hl=en&oi=scholarr},
  urldate = {2025-11-04}
}

@article{xuMentalLLMLeveragingLarge2024,
  title = {Mental-{{LLM}}: {{Leveraging Large Language Models}} for {{Mental Health Prediction}} via {{Online Text Data}}},
  shorttitle = {Mental-{{LLM}}},
  author = {Xu, Xuhai and Yao, Bingsheng and Dong, Yuanzhe and Gabriel, Saadia and Yu, Hong and Hendler, James and Ghassemi, Marzyeh and Dey, Anind K. and Wang, Dakuo},
  year = 2024,
  month = mar,
  journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  volume = {8},
  number = {1},
  pages = {1--32},
  issn = {2474-9567},
  doi = {10.1145/3643540},
  urldate = {2025-11-04},
  abstract = {Advances in large language models (LLMs) have empowered a variety of applications. However, there is still a significant gap in research when it comes to understanding and enhancing the capabilities of LLMs in the field of mental health. In this work, we present a comprehensive evaluation of multiple LLMs on various mental health prediction tasks via online text data, including Alpaca, Alpaca-LoRA, FLAN-T5, GPT-3.5, and GPT-4. We conduct a broad range of experiments, covering zero-shot prompting, few-shot prompting, and instruction fine-tuning. The results indicate a promising yet limited performance of LLMs with zero-shot and few-shot prompt designs for mental health tasks. More importantly, our experiments show that instruction finetuning can significantly boost the performance of LLMs for all tasks simultaneously. Our best-finetuned models, Mental-Alpaca and Mental-FLAN-T5, outperform the best prompt design of GPT-3.5 (25 and 15 times bigger) by 10.9\% on balanced accuracy and the best of GPT-4 (250 and 150 times bigger) by 4.8\%. They further perform on par with the state-of-the-art task-specific language model. We also conduct an exploratory case study on LLMs' capability on mental health reasoning tasks, illustrating the promising capability of certain models such as GPT-4. We summarize our findings into a set of action guidelines for potential methods to enhance LLMs' capability for mental health tasks. Meanwhile, we also emphasize the important limitations before achieving deployability in real-world mental health settings, such as known racial and gender bias. We highlight the important ethical risks accompanying this line of research.},
  langid = {english}
}

@misc{xuSurgWoundBenchBenchmarkSurgical2025,
  title = {{{SurgWound-Bench}}: {{A Benchmark}} for {{Surgical Wound Diagnosis}}},
  shorttitle = {{{SurgWound-Bench}}},
  author = {Xu, Jiahao and Yin, Changchang and Chatzipanagiotou, Odysseas and Tsilimigras, Diamantis and Clear, Kevin and Yao, Bingsheng and Wang, Dakuo and Pawlik, Timothy and Zhang, Ping},
  year = 2025,
  month = aug,
  number = {arXiv:2508.15189},
  eprint = {2508.15189},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2508.15189},
  urldate = {2025-11-04},
  abstract = {Surgical site infection (SSI) is one of the most common and costly healthcare-associated infections and and surgical wound care remains a significant clinical challenge in preventing SSIs and improving patient outcomes. While recent studies have explored the use of deep learning for preliminary surgical wound screening, progress has been hindered by concerns over data privacy and the high costs associated with expert annotation. Currently, no publicly available dataset or benchmark encompasses various types of surgical wounds, resulting in the absence of an open-source Surgical-Wound screening tool. To address this gap: (1) we present SurgWound, the first open-source dataset featuring a diverse array of surgical wound types. It contains 697 surgical wound images annotated by 3 professional surgeons with eight fine-grained clinical attributes. (2) Based on SurgWound, we introduce the first benchmark for surgical wound diagnosis, which includes visual question answering (VQA) and report generation tasks to comprehensively evaluate model performance. (3) Furthermore, we propose a three-stage learning framework, WoundQwen, for surgical wound diagnosis. In the first stage, we employ five independent MLLMs to accurately predict specific surgical wound characteristics. In the second stage, these predictions serve as additional knowledge inputs to two MLLMs responsible for diagnosing outcomes, which assess infection risk and guide subsequent interventions. In the third stage, we train a MLLM that integrates the diagnostic results from the previous two stages to produce a comprehensive report. This three-stage framework can analyze detailed surgical wound characteristics and provide subsequent instructions to patients based on surgical images, paving the way for personalized wound care, timely intervention, and improved patient outcomes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing}
}

@article{yangTalk2CareLLMbasedVoice2024,
  title = {{{Talk2Care}}: {{An LLM-based Voice Assistant}} for {{Communication}} between {{Healthcare Providers}} and {{Older Adults}}},
  shorttitle = {{{Talk2Care}}},
  author = {Yang, Ziqi and Xu, Xuhai and Yao, Bingsheng and Rogers, Ethan and Zhang, Shao and Intille, Stephen and Shara, Nawar and Gao, Guodong Gordon and Wang, Dakuo},
  year = 2024,
  month = may,
  journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  volume = {8},
  number = {2},
  pages = {1--35},
  issn = {2474-9567},
  doi = {10.1145/3659625},
  urldate = {2025-11-04},
  abstract = {Despite the plethora of telehealth applications to assist home-based older adults and healthcare providers, basic messaging and phone calls are still the most common communication methods, which suffer from limited availability, information loss, and process inefficiencies. One promising solution to facilitate patient-provider communication is to leverage large language models (LLMs) with their powerful natural conversation and summarization capability. However, there is a limited understanding of LLMs' role during the communication. We first conducted two interview studies with both older adults (N=10) and healthcare providers (N=9) to understand their needs and opportunities for LLMs in patient-provider asynchronous communication. Based on the insights, we built an LLM-powered communication system, Talk2Care, and designed interactive components for both groups: (1) For older adults, we leveraged the convenience and accessibility of voice assistants (VAs) and built an LLM-powered conversational interface for effective information collection. (2) For health providers, we built an LLM-based dashboard to summarize and present important health information based on older adults' conversations with the VA. We further conducted two user studies with older adults and providers to evaluate the usability of the system. The results showed that Talk2Care could facilitate the communication process, enrich the health information collected from older adults, and considerably save providers' efforts and time. We envision our work as an initial exploration of LLMs' capability in the intersection of healthcare and interpersonal communication.},
  langid = {english}
}

@misc{yangWishThereWere2024,
  title = {"{{I Wish There Were}} an {{AI}}": {{Challenges}} and {{AI Potential}} in {{Cancer Patient-Provider Communication}}},
  shorttitle = {"{{I Wish There Were}} an {{AI}}"},
  author = {Yang, Ziqi and Xu, Xuhai and Yao, Bingsheng and Li, Jiachen and Bagdasarian, Jennifer and Gao, Guodong and Wang, Dakuo},
  year = 2024,
  month = apr,
  number = {arXiv:2404.13409},
  eprint = {2404.13409},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2404.13409},
  urldate = {2025-11-04},
  abstract = {Patient-provider communication has been crucial to cancer patients' survival after their cancer treatments. However, the research community and patients themselves often overlook the communication challenges after cancer treatments as they are overshadowed by the severity of the patient's illness and the variety and rarity of the cancer disease itself. Meanwhile, the recent technical advances in AI, especially in Large Language Models (LLMs) with versatile natural language interpretation and generation ability, demonstrate great potential to support communication in complex real-world medical situations. By interviewing six healthcare providers and eight cancer patients, our goal is to explore the providers' and patients' communication barriers in the post-cancer treatment recovery period, their expectations for future communication technologies, and the potential of AI technologies in this context. Our findings reveal several challenges in current patient-provider communication, including the knowledge and timing gaps between cancer patients and providers, their collaboration obstacles, and resource limitations. Moreover, based on providers' and patients' needs and expectations, we summarize a set of design implications for intelligent communication systems, especially with the power of LLMs. Our work sheds light on the design of future AI-powered systems for patient-provider communication under high-stake and high-uncertainty situations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Human-Computer Interaction}
}

@misc{yaoAreHumanExplanations2023,
  title = {Are {{Human Explanations Always Helpful}}? {{Towards Objective Evaluation}} of {{Human Natural Language Explanations}}},
  shorttitle = {Are {{Human Explanations Always Helpful}}?},
  author = {Yao, Bingsheng and Sen, Prithviraj and Popa, Lucian and Hendler, James and Wang, Dakuo},
  year = 2023,
  month = may,
  number = {arXiv:2305.03117},
  eprint = {2305.03117},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.03117},
  urldate = {2025-11-04},
  abstract = {Human-annotated labels and explanations are critical for training explainable NLP models. However, unlike human-annotated labels whose quality is easier to calibrate (e.g., with a majority vote), human-crafted free-form explanations can be quite subjective. Before blindly using them as ground truth to train ML models, a vital question needs to be asked: How do we evaluate a human-annotated explanation's quality? In this paper, we build on the view that the quality of a human-annotated explanation can be measured based on its helpfulness (or impairment) to the ML models' performance for the desired NLP tasks for which the annotations were collected. In comparison to the commonly used Simulatability score, we define a new metric that can take into consideration the helpfulness of an explanation for model performance at both fine-tuning and inference. With the help of a unified dataset format, we evaluated the proposed metric on five datasets (e.g., e-SNLI) against two model architectures (T5 and BART), and the results show that our proposed metric can objectively evaluate the quality of human-annotated explanations, while Simulatability falls short.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{yaoDPRFGeneralizableDynamic2025,
  title = {{{DPRF}}: {{A Generalizable Dynamic Persona Refinement Framework}} for {{Optimizing Behavior Alignment Between Personalized LLM Role-Playing Agents}} and {{Humans}}},
  shorttitle = {{{DPRF}}},
  author = {Yao, Bingsheng and Sun, Bo and Dong, Yuanzhe and Lu, Yuxuan and Wang, Dakuo},
  year = 2025,
  month = oct,
  number = {arXiv:2510.14205},
  eprint = {2510.14205},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2510.14205},
  urldate = {2025-11-04},
  abstract = {The emerging large language model role-playing agents (LLM RPAs) aim to simulate individual human behaviors, but the persona fidelity is often undermined by manually-created profiles (e.g., cherry-picked information and personality characteristics) without validating the alignment with the target individuals. To address this limitation, our work introduces the Dynamic Persona Refinement Framework (DPRF). DPRF aims to optimize the alignment of LLM RPAs' behaviors with those of target individuals by iteratively identifying the cognitive divergence, either through free-form or theory-grounded, structured analysis, between generated behaviors and human ground truth, and refining the persona profile to mitigate these divergences. We evaluate DPRF with five LLMs on four diverse behavior-prediction scenarios: formal debates, social media posts with mental health issues, public interviews, and movie reviews. DPRF can consistently improve behavioral alignment considerably over baseline personas and generalizes across models and scenarios. Our work provides a robust methodology for creating high-fidelity persona profiles and enhancing the validity of downstream applications, such as user simulation, social studies, and personalized AI.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@misc{yaoExploringCollaborationBreakdowns2025,
  title = {Exploring {{Collaboration Breakdowns Between Provider Teams}} and {{Patients}} in {{Post-Surgery Care}}},
  author = {Yao, Bingsheng and Zhao, Menglin and Zhang, Zhan and Wang, Pengqi and Chester, Emma G. and Yin, Changchang and Li, Tianshi and Mishra, Varun and Padilla, Lace and Chatzipanagiotou, Odysseas and Pawlik, Timothy and Zhang, Ping and Cao, Weidan and Wang, Dakuo},
  year = 2025,
  month = sep,
  number = {arXiv:2509.23509},
  eprint = {2509.23509},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2509.23509},
  urldate = {2025-11-04},
  abstract = {Post-surgery care involves ongoing collaboration between provider teams and patients, which starts from post-surgery hospitalization through home recovery after discharge. While prior HCI research has primarily examined patients' challenges at home, less is known about how provider teams coordinate discharge preparation and care handoffs, and how breakdowns in communication and care pathways may affect patient recovery. To investigate this gap, we conducted semi-structured interviews with 13 healthcare providers and 4 patients in the context of gastrointestinal (GI) surgery. We found coordination boundaries between in- and out-patient teams, coupled with complex organizational structures within teams, impeded the "invisible work" of preparing patients' home care plans and triaging patient information. For patients, these breakdowns resulted in inadequate preparation for home transition and fragmented self-collected data, both of which undermine timely clinical decision-making. Based on these findings, we outline design opportunities to formalize task ownership and handoffs, contextualize co-temporal signals, and align care plans with home resources.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Human-Computer Interaction}
}

@misc{yaoLensHumanHumanCollaboration2025,
  title = {Through the {{Lens}} of {{Human-Human Collaboration}}: {{A Configurable Research Platform}} for {{Exploring Human-Agent Collaboration}}},
  shorttitle = {Through the {{Lens}} of {{Human-Human Collaboration}}},
  author = {Yao, Bingsheng and Chen, Jiaju and Chen, Chaoran and Wang, April and Li, Toby Jia-jun and Wang, Dakuo},
  year = 2025,
  month = sep,
  number = {arXiv:2509.18008},
  eprint = {2509.18008},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2509.18008},
  urldate = {2025-11-04},
  abstract = {Intelligent systems have traditionally been designed as tools rather than collaborators, often lacking critical characteristics that collaboration partnerships require. Recent advances in large language model (LLM) agents open new opportunities for human-LLM-agent collaboration by enabling natural communication and various social and cognitive behaviors. Yet it remains unclear whether principles of computer-mediated collaboration established in HCI and CSCW persist, change, or fail when humans collaborate with LLM agents. To support systematic investigations of these questions, we introduce an open and configurable research platform for HCI researchers. The platform's modular design allows seamless adaptation of classic CSCW experiments and manipulation of theory-grounded interaction controls. We demonstrate the platform's effectiveness and usability through two case studies: (1) re-implementing the classic human-human-collaboration task Shape Factory as a between-subject human-agent-collaboration experiment with 16 participants, and (2) a participatory cognitive walkthrough with five HCI researchers to refine workflows and interfaces for experiment setup and analysis.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction}
}

@misc{yaoMoreModalityMore2025,
  title = {More {{Modality}}, {{More AI}}: {{Exploring Design Opportunities}} of {{AI-Based Multi-modal Remote Monitoring Technologies}} for {{Early Detection}} of {{Mental Health Sequelae}} in {{Youth Concussion Patients}}},
  shorttitle = {More {{Modality}}, {{More AI}}},
  author = {Yao, Bingsheng and Zhao, Menglin and Sun, Yuling and Cao, Weidan and Yin, Changchang and Intille, Stephen and Xu, Xuhai and Zhang, Ping and Yang, Jingzhen and Wang, Dakuo},
  year = 2025,
  month = apr,
  number = {arXiv:2502.03732},
  eprint = {2502.03732},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.03732},
  urldate = {2025-11-04},
  abstract = {Anxiety, depression, and suicidality are common mental health sequelae following concussion in youth patients, often exacerbating concussion symptoms and prolonging recovery. Despite the critical need for early detection of these mental health symptoms, clinicians often face challenges in accurately collecting patients' mental health data and making clinical decision-making in a timely manner. Today's remote patient monitoring (RPM) technologies offer opportunities to objectively monitor patients' activities, but they were not specifically designed for youth concussion patients; moreover, the large amount of data collected by RPM technologies may also impose significant workloads on clinicians to keep up with and use the data. To address these gaps, we employed a three-stage study consisting of a formative study, interface design, and design evaluation. We first conducted a formative study through semi-structured interviews with six highly professional concussion clinicians and identified clinicians' key challenges in remotely collecting patient information and accessing patient treatment compliance. Subsequently, we proposed preliminary clinician-facing interface designs with the integration of AI-based RPM technologies (AI-RPM), followed by design evaluation sessions with highly professional concussion clinicians. Clinicians underscored the value of integrating multi-modal AI-RPM technologies to support their decision-making while emphasizing the importance of customizable interfaces through collaborative design and multiple responsible design considerations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Human-Computer Interaction}
}

@inproceedings{yaoMoreSamplesMore2024a,
  title = {More Samples or More Prompts? Exploring Effective Few-Shot in-Context Learning for {{LLMs}} with in-Context Sampling},
  shorttitle = {More Samples or More Prompts?},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{NAACL}} 2024},
  author = {Yao, Bingsheng and Chen, Guiming and Zou, Ruishi and Lu, Yuxuan and Li, Jiachen and Zhang, Shao and Sang, Yisi and Liu, Sijia and Hendler, James and Wang, Dakuo},
  year = 2024,
  pages = {1772--1790},
  url = {https://aclanthology.org/2024.findings-naacl.115/},
  urldate = {2025-11-04}
}

@misc{yaoWhyInterdisciplinaryTeams2025,
  title = {Why {{Interdisciplinary Teams Fail}}: {{A Systematic Analysis With Activity Theory}} in {{Clinical AI Collaboration}}},
  shorttitle = {Why {{Interdisciplinary Teams Fail}}},
  author = {Yao, Bingsheng and Du, Yao and Fu, Yue and Xu, Xuhai and Gao, Yanjun and Yu, Hong and Wang, Dakuo},
  year = 2025,
  month = jun,
  number = {arXiv:2410.00174},
  eprint = {2410.00174},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.00174},
  urldate = {2025-11-04},
  abstract = {Advanced AI technologies are increasingly integrated into clinical domains to advance patient care. The design and development of clinical AI technologies necessitate seamless collaboration between clinical and technical experts. Yet, such interdisciplinary teams are often unsuccessful, with a lack of systematic analysis of collaboration barriers and coping strategies. This work examines two clinical AI collaborations in the context of speech-language pathology via semi-structured interviews with six clinical and seven technical experts. Using Activity Theory (AT) as our analytical lens, we systematically investigate persistent knowledge gaps in mismatched data coding themes and specialized languages, and also highlight how clinical data can act as boundary objects and human knowledge brokers to alleviate these challenges. Our work underscores the benefits of leveraging analytical frameworks like AT to systematically examine interdisciplinary teams' collaborative work and provide meaningful insights on best practices in future collaboration.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Human-Computer Interaction}
}

@inproceedings{yinSepsisCalcIntegratingClinical2025,
  title = {{{SepsisCalc}}: {{Integrating Clinical Calculators}} into {{Early Sepsis Prediction}} via {{Dynamic Temporal Graph Construction}}},
  shorttitle = {{{SepsisCalc}}},
  booktitle = {Proceedings of the 31st {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining V}}.1},
  author = {Yin, Changchang and Fu, Shihan and Yao, Bingsheng and Pham, Thai-Hoang and Cao, Weidan and Wang, Dakuo and Caterino, Jeffrey and Zhang, Ping},
  year = 2025,
  month = jul,
  pages = {2779--2790},
  publisher = {ACM},
  address = {Toronto ON Canada},
  doi = {10.1145/3690624.3709402},
  urldate = {2025-11-04},
  isbn = {979-8-4007-1245-6},
  langid = {english}
}

@inproceedings{yinSepsisLabEarlySepsis2024,
  title = {{{SepsisLab}}: {{Early Sepsis Prediction}} with {{Uncertainty Quantification}} and {{Active Sensing}}},
  shorttitle = {{{SepsisLab}}},
  booktitle = {Proceedings of the 30th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Yin, Changchang and Chen, Pin-Yu and Yao, Bingsheng and Wang, Dakuo and Caterino, Jeffrey and Zhang, Ping},
  year = 2024,
  month = aug,
  pages = {6158--6168},
  publisher = {ACM},
  address = {Barcelona Spain},
  doi = {10.1145/3637528.3671586},
  urldate = {2025-11-04},
  isbn = {979-8-4007-0490-1},
  langid = {english}
}

@article{zhangDoctorPupilVirtualReality2025,
  title = {{{DoctorPupil}}: A Virtual Reality System for {{Parkinson}}'s Diagnosis through Task-Evoked Pupil Response},
  shorttitle = {{{DoctorPupil}}},
  author = {Zhang, Xucheng and Wan, Zhirong and Zhao, Jing and Li, Xinjin and Liu, Anfeng and Fan, Xiangmin and Sun, Wei and Tian, Feng and Wang, Dakuo},
  year = 2025,
  journal = {IEEE Journal of Biomedical and Health Informatics},
  publisher = {IEEE},
  url = {https://ieeexplore.ieee.org/abstract/document/10849579/},
  urldate = {2025-11-04}
}

@article{zhangGeoDeepShovelPlatformBuilding2023,
  title = {{{{\textsc{GeoDeepShovel}}}} : {{A}} Platform for Building Scientific Database from Geoscience Literature with {{{\textsc{AI}}}} Assistance},
  author = {Zhang, Shao and Xu, Hui and Jia, Yuting and Wen, Ying and Wang, Dakuo and Fu, Luoyi and Wang, Xinbing and Zhou, Chenghu},
  year = 2023,
  month = oct,
  journal = {Geoscience Data Journal},
  volume = {10},
  number = {4},
  pages = {519--537},
  issn = {2049-6060, 2049-6060},
  doi = {10.1002/gdj3.186},
  urldate = {2025-11-04},
  abstract = {Abstract             With the rapid development of big data science, the research paradigm in the field of geosciences has also begun to shift to big data-driven scientific discovery. Researchers need to read a huge amount of literature to locate, extract and aggregate relevant results and data that are published and stored in PDF format for building a scientific database to support the big data-driven discovery. In this paper, based on the findings of a study about how geoscientists annotate literature and extract and aggregate data, we proposed GeoDeepShovel, a publicly available AI-assisted data extraction system to support their needs. GeoDeepShovel leverages state-of-the-art neural network models to support researcher(s) easily and accurately annotate papers (in the PDF format) and extract data from tables, figures, maps, etc., in a human--AI collaboration manner. As a part of the Deep-Time Digital Earth (DDE) program, GeoDeepShovel has been deployed for 8~months, and there are already 400 users from 44 geoscience research teams within the DDE program using it to construct scientific databases on a daily basis, and more than 240 projects and 50,000 documents have been processed for building scientific databases.},
  langid = {english}
}

@article{zhangItsaFairGame2023,
  title = {It'sa Fair Game, or Is It? Examining How Users Navigate Disclosure Risks and Benefits When Using Llm-Based Conversational Agents},
  shorttitle = {It'sa Fair Game, or Is It?},
  author = {Zhang, Zhiping and Jia, Michelle and Yao, Bingsheng and Das, Sauvik and Lerner, Ada and Wang, Dakuo and Li, Tianshi},
  year = 2023,
  journal = {arXiv preprint arXiv:2309.11653},
  eprint = {2309.11653},
  url = {https://hankhplee.com/papers/llm_privacy.pdf},
  urldate = {2025-11-04},
  archiveprefix = {arXiv}
}

@misc{zhangMutualTheoryMind2024,
  title = {Mutual {{Theory}} of {{Mind}} in {{Human-AI Collaboration}}: {{An Empirical Study}} with {{LLM-driven AI Agents}} in a {{Real-time Shared Workspace Task}}},
  shorttitle = {Mutual {{Theory}} of {{Mind}} in {{Human-AI Collaboration}}},
  author = {Zhang, Shao and Wang, Xihuai and Zhang, Wenhao and Chen, Yongshan and Gao, Landi and Wang, Dakuo and Zhang, Weinan and Wang, Xinbing and Wen, Ying},
  year = 2024,
  month = sep,
  number = {arXiv:2409.08811},
  eprint = {2409.08811},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.08811},
  urldate = {2025-11-04},
  abstract = {Theory of Mind (ToM) significantly impacts human collaboration and communication as a crucial capability to understand others. When AI agents with ToM capability collaborate with humans, Mutual Theory of Mind (MToM) arises in such human-AI teams (HATs). The MToM process, which involves interactive communication and ToM-based strategy adjustment, affects the team's performance and collaboration process. To explore the MToM process, we conducted a mixed-design experiment using a large language model-driven AI agent with ToM and communication modules in a real-time shared-workspace task. We find that the agent's ToM capability does not significantly impact team performance but enhances human understanding of the agent and the feeling of being understood. Most participants in our study believe verbal communication increases human burden, and the results show that bidirectional communication leads to lower HAT performance. We discuss the results' implications for designing AI agents that collaborate with humans in real-time shared workspace tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Multiagent Systems}
}

@article{zhangResponsibleAIHealthcare2023,
  title = {Responsible {{AI}} in Healthcare: Opportunities, Challenges, and Best Practices},
  shorttitle = {Responsible {{AI}} in Healthcare},
  author = {Zhang, Renwen and Zhang, Zhan and Wang, Dakuo and Liu, Ziwei},
  year = 2023,
  journal = {Frontiers in Computer Science},
  volume = {5},
  pages = {1265902},
  publisher = {Frontiers Media SA},
  url = {https://www.frontiersin.org/journals/computer-science/articles/10.3389/fcomp.2023.1265902/full},
  urldate = {2025-11-04}
}

@article{zhangSecretUseLarge2025,
  title = {Secret {{Use}} of {{Large Language Model}} ({{LLM}})},
  author = {Zhang, Zhiping and Shen, Chenxinran and Yao, Bingsheng and Wang, Dakuo and Li, Tianshi},
  year = 2025,
  month = may,
  journal = {Proceedings of the ACM on Human-Computer Interaction},
  volume = {9},
  number = {2},
  pages = {1--26},
  issn = {2573-0142},
  doi = {10.1145/3711061},
  urldate = {2025-11-04},
  abstract = {The advancements of Large Language Models (LLMs) have decentralized the responsibility for the transparency of AI usage. Specifically, LLM users are now encouraged or required to disclose the use of LLM-generated content for varied types of real-world tasks. However, an emerging phenomenon, users'               secret use of LLM               , raises challenges in ensuring end users adhere to the transparency requirement. Our study used mixed-methods with an exploratory survey (125 real-world secret use cases reported) and a controlled experiment among 300 users to investigate the contexts and causes behind the secret use of LLMs. We found that such secretive behavior is often triggered by certain tasks, transcending demographic and personality differences among users. Task types were found to affect users' intentions to use secretive behavior, primarily through influencing perceived external judgment regarding LLM usage. Our results yield important insights for future work on designing interventions to encourage more transparent disclosure of the use of LLMs or other AI technologies.},
  langid = {english}
}

@misc{zhangSeeThinkAct2025,
  title = {See, {{Think}}, {{Act}}: {{Online Shopper Behavior Simulation}} with {{VLM Agents}}},
  shorttitle = {See, {{Think}}, {{Act}}},
  author = {Zhang, Yimeng and Gesi, Jiri and Xue, Ran and Wang, Tian and Wang, Ziyi and Lu, Yuxuan and Zhan, Sinong and Zeng, Huimin and Cui, Qingjun and Guo, Yufan and Huang, Jing and Shah, Mubarak and Wang, Dakuo},
  year = 2025,
  month = oct,
  number = {arXiv:2510.19245},
  eprint = {2510.19245},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2510.19245},
  urldate = {2025-11-04},
  abstract = {LLMs have recently demonstrated strong potential in simulating online shopper behavior. Prior work has improved action prediction by applying SFT on action traces with LLM-generated rationales, and by leveraging RL to further enhance reasoning capabilities. Despite these advances, current approaches rely on text-based inputs and overlook the essential role of visual perception in shaping human decision-making during web GUI interactions. In this paper, we investigate the integration of visual information, specifically webpage screenshots, into behavior simulation via VLMs, leveraging OPeRA dataset. By grounding agent decision-making in both textual and visual modalities, we aim to narrow the gap between synthetic agents and real-world users, thereby enabling more cognitively aligned simulations of online shopping behavior. Specifically, we employ SFT for joint action prediction and rationale generation, conditioning on the full interaction context, which comprises action history, past HTML observations, and the current webpage screenshot. To further enhance reasoning capabilities, we integrate RL with a hierarchical reward structure, scaled by a difficulty-aware factor that prioritizes challenging decision points. Empirically, our studies show that incorporating visual grounding yields substantial gains: the combination of text and image inputs improves exact match accuracy by more than 6\% over text-only inputs. These results indicate that multi-modal grounding not only boosts predictive accuracy but also enhances simulation fidelity in visually complex environments, which captures nuances of human attention and decision-making that text-only agents often miss. Finally, we revisit the design space of behavior simulation frameworks, identify key methodological limitations, and propose future research directions toward building efficient and effective human behavior simulators.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Computer Science - Multimedia}
}

@misc{zhaoDesigningAITools2025,
  title = {Designing {{AI Tools}} for {{Clinical Care Teams}} to {{Support Serious Illness Conversations}} with {{Older Adults}} in the {{Emergency Department}}},
  author = {Zhao, Menglin and Yong, Zhuorui and Guan, Ruijia and Chang, Kai-Wei and Haimovich, Adrian and Ouchi, Kei and Bickmore, Timothy and Yao, Bingsheng and Wang, Dakuo and Desai, Smit},
  year = 2025,
  month = may,
  number = {arXiv:2506.00241},
  eprint = {2506.00241},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.00241},
  urldate = {2025-11-04},
  abstract = {Serious illness conversations (SICs), discussions between clinical care teams and patients with serious, life-limiting illnesses about their values, goals, and care preferences, are critical for patient-centered care. Without these conversations, patients often receive aggressive interventions that may not align with their goals. Clinical care teams face significant barriers when conducting serious illness conversations with older adult patients in Emergency Department (ED) settings, where most older adult patients lack documented treatment goals. To understand current practices and identify AI support opportunities, we conducted interviews with two domain experts and nine ED clinical care team members. Through thematic analysis, we characterized a four-phase serious illness conversation workflow (identification, preparation, conduction, documentation) and identified key needs and challenges at each stage. Clinical care teams struggle with fragmented EHR data access, time constraints, emotional preparation demands, and documentation burdens. While participants expressed interest in AI tools for information synthesis, conversational support, and automated documentation, they emphasized preserving human connection and clinical autonomy. We present design guidelines for AI tools supporting SIC workflows that fit within existing clinical practices. This work contributes empirical understanding of ED-based serious illness conversations and provides design considerations for AI in high-stakes clinical environments.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction}
}

@inproceedings{zhuFeatureEngineeringHuman2024,
  title = {Towards {{Feature Engineering}} with {{Human}} and {{AI}}'s {{Knowledge}}: {{Understanding Data Science Practitioners}}' {{Perceptions}} in {{Human}}\&{{AI-Assisted Feature Engineering Design}}},
  shorttitle = {Towards {{Feature Engineering}} with {{Human}} and {{AI}}'s {{Knowledge}}},
  booktitle = {Designing {{Interactive Systems Conference}}},
  author = {Zhu, Qian and Wang, Dakuo and Ma, Shuai and Wang, April Yi and Chen, Zixin and Khurana, Udayan and Ma, Xiaojuan},
  year = 2024,
  month = jul,
  pages = {1789--1804},
  publisher = {ACM},
  address = {Copenhagen Denmark},
  doi = {10.1145/3643834.3661517},
  urldate = {2025-11-04},
  isbn = {979-8-4007-0583-0},
  langid = {english}
}

@misc{zouDesigningEvaluatingSampling2024,
  title = {Designing and {{Evaluating Sampling Strategies}} for {{Multiple-Forecast Visualization}} ({{MFV}})},
  author = {Zou, Ruishi and Wu, Siyi and Yao, Bingsheng and Wang, Dakuo and Padilla, Lace},
  year = 2024,
  month = nov,
  number = {arXiv:2411.02576},
  eprint = {2411.02576},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2411.02576},
  urldate = {2025-11-04},
  abstract = {With the growing availability of quantitative forecasts from various sources, effectively communicating these multiple forecasts has become increasingly crucial. Recent advances have explored using Multiple-Forecast Visualizations (MFVs) to display multiple time-series forecasts. However, how to systematically sample from a pool of disparate forecasts to create MFVs that effectively facilitate decision-making requires further investigation. To address this challenge, we examine two cluster-based sampling strategies for creating MFVs and three designs for visualizing them to assist people in decision-making with forecasts. Through two online studies (Experiment 1 n = 711 and Experiment 2 n = 400) and over 15 decision-making-related metrics, we evaluated participants' perceptions of eight visualization designs using historical COVID-19 forecasts as a test bed. Our findings revealed that one sampling method significantly enhanced participants' ability to predict future outcomes, thereby reducing their surprise when confronted with the actual outcomes. Importantly, since no approach excels in all metrics, we advise choosing different visualization designs based on communication goals. Furthermore, qualitative response data demonstrate a correlation between response consistency and people's inclination to extrapolate from the forecast segment of the visualization. This research offers insights into how to improve visualizations of multiple forecasts using an automated and empirically validated technique for selecting forecasts that outperform common techniques on several key metrics and reduce overplotting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Human-Computer Interaction}
}
